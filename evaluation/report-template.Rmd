---
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 1
    theme: journal
title: "EpiExpert Crowd-Forecasting Evaluation - Europe"
# toc: true
# toc_float: true
# toc_collapsed: true
# toc_depth: 3
---

```{r setup, include=FALSE}
library(covid.ecdc.forecasts)
library(scoringutils)
library(ggplot2)
library(dplyr)
library(DT)
library(knitr)
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE)

render_ranking <- TRUE

```

<br>

This is an evaluation of forecasts of Covid-19 case and death numbers submitted by the [Epiforecasts team](https://github.com/epiforecasts) to the [European COVID-19 Forecast Hub](https://covid19forecasthub.eu/) ([European Forecast Hub Github page](https://github.com/epiforecasts/covid19-forecast-hub-europe)). These include the crowd forecasts made through the [crowdforecastr app](https://cmmid-lshtm.shinyapps.io/crowd-forecast/) as well as a model created using [EpiNow2](https://github.com/epiforecasts/epinow2). 

This report is intended as a basic evaluation of predictions that helps forecasters to better understand their performance. The structure and visualisations are likely subject to change in the future and we cannot rule out any mistakes. You can learn more and provide feedback by creating an issue on our [github repository](https://github.com/epiforecasts/europe-covid-forecast)

```{r load-data}
# load all data ----------------------------------------------------------------
root_dir <- here::here("crowd-forecast", "processed-forecast-data")
file_paths_forecast <- here::here(root_dir, list.files(root_dir))

prediction_data <- purrr::map_dfr(file_paths_forecast, 
                                  .f = function(x) {
                                    data <- data.table::fread(x) %>%
                                      dplyr::mutate(target_end_date = as.Date(target_end_date), 
                                                    submission_date = as.Date(submission_date), 
                                                    forecast_date = as.Date(forecast_date))
                                  }) %>%
  dplyr::mutate(target_type = ifelse(grepl("death", target), "death", "case")) %>%
  dplyr::rename(prediction = value) %>%
  dplyr::mutate(forecast_date = as.Date(submission_date)) %>%
  dplyr::rename(model = board_name) %>%
  dplyr::filter(type == "quantile") %>%
  dplyr::select(location, location_name, forecast_date, quantile, prediction, model, target_end_date, horizon, target, target_type)

# filter forecasters such that only those are kept that have submitted forecasts
# in the last four weeks
keep_fc <- prediction_data %>%
  dplyr::filter(forecast_date > (Sys.Date() - 4 * 7)) %>%
  dplyr::pull(model) %>%
  unique()

prediction_data <- dplyr::filter(prediction_data, 
                                 model %in% keep_fc)

files <- list.files(here::here("data-raw"))
file_paths <- here::here("data-raw", files[grepl("weekly-incident", files)])
names(file_paths) <- c("case", "death")

truth_data <- purrr::map_dfr(file_paths, readr::read_csv, .id = "target_type") %>%
  dplyr::rename(true_value = value) %>%
  dplyr::mutate(target_end_date = as.Date(target_end_date)) %>%
  dplyr::arrange(location, target_type, target_end_date) 

data <- scoringutils::merge_pred_and_obs(prediction_data, truth_data, 
                                         join = "full")

# rename target type to target variable to conform to hub format
setnames(data, old = c("target_type"), new = c("target_variable"))
data[, target_variable := ifelse(target_variable == "case", "inc case", "inc death")]
```

<br>

---

# Forecaster ranking {.tabset}

Here is an overall ranking of all forecasters. The ranking is made according to relative skill. Relative skill is calculated by looking at all pairwise comparisons between forecasters in terms of the weighted interval score (WIS). See below for a more detailed explanation of the scoring metrics used. 'Overall' shows the complete ranking, 'latest' only spans the last 5-6 weeks of data. 'Detailed' represents the full data set that you can download for your own analysis. 

```{r ranking, include = FALSE, eval = render_ranking}
out <- NULL

tabtitle = "overall"
summarise_by = c("model")
filter_list = list()
out = c(out, knit_child(here::here("evaluation", "template-ranking-table.Rmd")))

tabtitle = "latest"
summarise_by = c("model")
cut_off <- latest_weekday(Sys.Date()) - 5 * 7
filter_list = list(paste0("forecast_date >= '", cut_off, "'"))
out = c(out, knit_child(here::here("evaluation", "template-ranking-table.Rmd")))

tabtitle = "Cases"
summarise_by = c("model", "target_variable")
filter_list = list('target_variable == "inc case"')
out = c(out, knit_child(here::here("evaluation", "template-ranking-table.Rmd"),
        quiet = TRUE))

tabtitle = "Deaths"
summarise_by = c("model", "target_variable")
filter_list = list('target_variable == "inc death"')
out = c(out, knit_child(here::here("evaluation", "template-ranking-table.Rmd"),
        quiet = TRUE))

# tabtitle = "Germany"
# summarise_by = c("model", "location_name")
# filter_list = list('location_name == "Germany"')
# out = c(out, knit_child(here::here("evaluation", "template-ranking-table.Rmd")),
#         quiet = TRUE)
# 
# tabtitle = "Poland"
# summarise_by = c("model", "location_name")
# filter_list = list('location_name == "Poland"')
# out = c(out, knit_child(here::here("evaluation", "template-ranking-table.Rmd")),
#         quiet = TRUE)

tabtitle = "overall by horizon"
summarise_by = c("model", "horizon")
filter_list = NULL
out = c(out, knit_child(here::here("evaluation", "template-ranking-table.Rmd"),
        quiet = TRUE))

tabtitle = "Detailed"
summarise_by = NULL
filter_list = NULL
out = c(out, knit_child(here::here("evaluation", "template-ranking-table.Rmd"),
        quiet = TRUE))

```

`r if (render_ranking) paste(knit(text = out), collapse = '\n\n---\n\n')`

# {.unlisted .unnumbered}

<br>

--- 

# Evaluation metrics

 - Relative skill is a metric based on the weighted interval score (WIS) that is using a 'pairwise comparison tournament'. All pairs of forecasters are compared against each other in terms of the weighted interval score. The mean score of both models based on the set of common targets for which both models have made a prediction are calculated to obtain mean score ratios. The relative skill is the geometric mean of these mean score ratios. Smaller values are better and a value smaller than one means that the model beats the average forecasting model. 
 - The weighted interval score is a proper scoring rule (meaning you can't cheat it) suited to scoring forecasts in an interval format. It has three components: sharpness, underprediction and overprediction. Sharpness is the width of your prediction interval. Over- and underprediction only come into play if the prediction interval does not cover the true value. They are the absolute value of the difference between the upper or lower bound of your prediction interval (depending on whether your forecast is too high or too low). 
 - coverage deviation is the average difference between nominal and empirical interval coverage. Say your 50 percent prediction interval covers only 20 percent of all true values, then your coverage deviation is 0.5 - 0.2 = -0.3. The coverage deviation value in the table is calculated by averaging over the coverage deviation calculated for all possible prediction intervals. If the value is negative you have covered less then you should. If it is positve, then your forecasts could be a little more confident. 
 - bias is a measure between -1 and 1 that expresses your tendency to underpredict (-1) or overpredict (1). In contrast to the over- and underprediction components of the WIS it is bound between -1 and 1 and cannot go to infinity. It is therefore less susceptible to outliers. 
 - aem is the absolute error of your median forecasts. A high aem means your median forecasts tend to be far away from the true values. 

# {.unlisted .unnumbered}

<br>

---

# Forecast visualisation {.tabset .tabset-fade}

This is a visualisation of all forecasts made so far. 

```{r forecast-vis, include = FALSE}

locations <- unique(data$location_name)
forecast_dates <- rev(as.character(unique(data$forecast_date[!is.na(data$forecast_date)])))
target_variables <- c("inc case", "inc death")

out <- NULL
out = c(out, knit_child(here::here("evaluation", "template-plot-forecasts.Rmd")))
```

`r paste(knit(text = out), collapse = '\n\n')`

# {.unlisted .unnumbered}

<br>

--- 

# Scores over time {.tabset .tabset-fade .tabset-dropdown}

Here you can see a visualisation of forecaster scores together next to the true observed values: 

```{r forecast-and-scores, include = FALSE}
out <- NULL
out = c(out, knit_child(here::here("evaluation", "template-scores-and-truth-time.Rmd")))
```

`r paste(knit(text = out), collapse = '\n\n')`

# {.unlisted .unnumbered}

<br>

--- 

# Ranks over time {.tabset .tabset-fade .tabset-dropdown}

This table shows you either your rank among all forecasters or the standardised rank. The standardised rank is computed as (100 - the forecaster percentile rank) among all forecasters for a given target and forecast date. What happens is basically this: Every forecaster gets assigned a rank (1 is the best and the worst equals the number of available forecasts for that date). This rank is then transformed to a scale from 1 to 100 such that 100 is best and 0 is worst. Ranks are determined based on the weighted interval scores.

```{r ranking-over-time, include = FALSE}
out <- NULL
out = c(out, knit_child(here::here("evaluation", "template-ranking-over-time.Rmd")))
```

`r paste(knit(text = out), collapse = '\n\n')`

# {.unlisted .unnumbered}

<br>

--- 
 
# WIS decomposition {.tabset .tabset-fade .tabset-dropdown}

The weighted interval score can be decomposed into three parts: sharpness (the amount of uncertainty around the forecast), overprediction and underprediction. This visualisation gives an impression of the distribution between these three forms of penalties for the different forecasters. 

```{r wis-components, include = FALSE}
out <- NULL
out = c(out, knit_child(here::here("evaluation", "template-wis-components.Rmd")))
``` 

`r paste(knit(text = out), collapse = '\n\n')`

# {.unlisted .unnumbered}

<br>

--- 

# Models and available forecasts

The following graphic gives an overview of the forecasters and models analysed and the number of forecasts they contributed. 

```{r show-avail-forecasts, results = 'asis'}
plot <- scoringutils::show_avail_forecasts(prediction_data,
                                           show_numbers = FALSE,
                                           make_x_factor = FALSE,
                                           summarise_by = c("forecast_date", 
                                                            "model", 
                                                            "horizon", 
                                                            "target_type"),
                                           legend_position = "bottom") + 
  labs(fill = "number of locations", x = "forecast date")
print(plot)
```

Most of the 'models' are human forecasters, but some are not: 

 - EpiExpert-ensemble is the ensemble that is formed as the mean of all human forecasts submitted
 - [EpiNow2](https://epiforecasts.io/EpiNow2/) is an exponential growth model that uses a time-varying Rt trajectory to predict latent infections, and then convolves these infections with estimated delays to observations, via a negative binomial model coupled with a day of the week effect. It makes limited assumptions and is not tuned to the specifities of Covid in Germany and Poland beyond epidemioligical details such as literature estimates of the generation time, incubation period and the population of each area. The method and underlying theory are under active development with more details available [here](https://epiforecasts.io/covid/methods).
 <!-- - EpiNow2-secondary is an EpiNow2 model that infers deaths from cases by convoluting case numbers with a delay distribution -->
 <!-- - Crowd-Rt-Forecast is an ensemble that is constructed by taking human crowd forecasts of Rt and calculating implied case numbers from that using EpiNow2 -->

